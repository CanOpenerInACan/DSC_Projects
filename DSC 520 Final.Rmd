---
title: 'Final Project: Predicting SAT Scores in NYC'
author: "David Lattimer"
date: "8/2/2020"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r include=FALSE}
setwd("C:/Users/David/Documents/DSC520-master/dsc520-master")
library(dplyr)
library(ggplot2)
library(caTools)
scores <- read.csv("finals/scores.csv")
census <- read.csv("finals/census.csv")
``` 

```{r include=FALSE}

#This is to clean the census data. We are going to be taking data from this and then putting it into our scores dataset so we can do what we want. We first will remove the stuff that we don't need, then we will work on finding what we need. We slice the data to essentially get 5 different data frames that only include the boroughs we want and also find income per capita in this chunk.

borough_census <- census$Borough
population <- census$TotalPop
income_per_cap <- census$IncomePerCap
poverty <- census$Poverty
unemployment <- census$Unemployment
census$total_money <- as.numeric(population)*income_per_cap
cleaned_census <- data.frame("Borough" = borough_census, "Population" = population, "Income per Capita" = income_per_cap, "Poverty" = poverty, "Unemployment" = unemployment, "Total Money" = census$total_money, check.names=FALSE)

bronx_census <- slice(na.omit(cleaned_census), 1:335)
bronx_total_pop <- sum(bronx_census$Population)
bronx_total_money <- sum(bronx_census$"Total Money")
bronx_income_per_cap <- bronx_total_money/bronx_total_pop

brooklyn_census <- slice(na.omit(cleaned_census), 336:1084)
brooklyn_total_population <- sum(brooklyn_census$Population)
brooklyn_total_money <- sum(brooklyn_census$"Total Money")
brooklyn_income_per_cap <- brooklyn_total_money/brooklyn_total_population

manhattan_census <- slice(na.omit(cleaned_census), 1085:1365)
manhattan_total_pop <- sum(manhattan_census$Population)
manhattan_total_money <- sum(manhattan_census$"Total Money")
manhattan_income_per_cap <- manhattan_total_money/manhattan_total_pop

queens_census <- slice(na.omit(cleaned_census), 1366:2011)
queens_total_pop <- sum(queens_census$Population)
queens_total_money <- sum(queens_census$"Total Money")
queens_income_per_cap <- queens_total_money/queens_total_pop

staten_census <- slice(na.omit(cleaned_census), 2012:2166)
staten_total_pop <- sum(staten_census$Population)
staten_total_money <- sum(staten_census$"Total Money")
staten_income_per_cap <- staten_total_money/staten_total_pop
```

```{r include=FALSE}

#This takes the separated datasets we set up earlier to only include boroughs we are looking at and then finds the poverty percent of that borough. We multiply the percent of people in a tract that are in poverty by the population to find the count of people that are in poverty in each census tract. We then add up all those in poverty and divide it by the total population of the borough to find the poverty percent of that borough.

bronx_census$"Poverty Count of Tract" <- bronx_census$Poverty*bronx_census$Population/100
poverty_percent_bronx <- sum(bronx_census$"Poverty Count of Tract")/sum(bronx_census$Population)*100

brooklyn_census$"Poverty Count of Tract" <- brooklyn_census$Poverty*brooklyn_census$Population/100
poverty_percent_brooklyn <- sum(brooklyn_census$"Poverty Count of Tract")/sum(brooklyn_census$Population)*100

manhattan_census$"Poverty Count of Tract" <- manhattan_census$Poverty*manhattan_census$Population/100
poverty_percent_manhattan <- sum(manhattan_census$"Poverty Count of Tract")/sum(manhattan_census$Population)*100

queens_census$"Poverty Count of Tract" <- queens_census$Poverty*queens_census$Population/100
poverty_percent_queens <- sum(queens_census$"Poverty Count of Tract")/sum(queens_census$Population)*100

staten_census$"Poverty Count of Tract" <- staten_census$Poverty*staten_census$Population/100
poverty_percent_staten <- sum(staten_census$"Poverty Count of Tract")/sum(staten_census$Population)*100
```

```{r include=FALSE}

#This is finding the unemployment rate of the borough. We multiply unemployment percent of the tract by the population to find how many people in the tract are unemployed. We then add up all the unemployed in the borough and divide it by the total population to find the unemployment percentage of that borough.

bronx_census$"Total Unemployed" <- bronx_census$Population*bronx_census$Unemployment/100
bronx_unemployment <- sum(bronx_census$"Total Unemployed")/sum(bronx_census$Population)*100

brooklyn_census$"Total Unemployed" <- brooklyn_census$Population*brooklyn_census$Unemployment/100
brooklyn_unemployment <- sum(brooklyn_census$"Total Unemployed")/sum(brooklyn_census$Population)*100

manhattan_census$"Total Unemployed" <- manhattan_census$Population*manhattan_census$Unemployment/100
manhattan_unemployment <- sum(manhattan_census$"Total Unemployed")/sum(manhattan_census$Population)*100

queens_census$"Total Unemployed" <- queens_census$Population*queens_census$Unemployment/100
queens_unemployment <- sum(queens_census$"Total Unemployed")/sum(queens_census$Population)*100

staten_census$"Total Unemployed" <- staten_census$Population*staten_census$Unemployment/100
staten_unemployment <- sum(staten_census$"Total Unemployed")/sum(staten_census$Population)*100
```

```{r include=FALSE}

#This cleans up the scores data set. It is straight forward because we are going to add data to this set from the census.

school_id <- scores$School.ID
zip_code <- scores$Zip.Code
borough <- scores$Borough
math_scores <- scores$Average.Score..SAT.Math.
writing_scores <- scores$Average.Score..SAT.Writing.
reading_scores <- scores$Average.Score..SAT.Reading.
percent_white <- scores$Percent.White
percent_black <- scores$Percent.Black
percent_hispanic <- scores$Percent.Hispanic
percent_asian <- scores$Percent.Asian
percent_tested <- scores$Percent.Tested
cleaned_scores <- data.frame("School ID" = school_id, "Zip Code" = zip_code, "Borough" = borough, "Percent White" = percent_white, "Percent Black" = percent_black, "Percent Hispanic" = percent_hispanic, "Percent Asian" = percent_asian, "Average Math Score" = math_scores, "Average Reading Score" = reading_scores, "Average Writing Score" =writing_scores, "Percent Tested" = percent_tested, check.names=FALSE)
```

```{r include=FALSE}

#This is to add all the variables we found above (income per capita, poverty percent and unemployment percent), and then combine the sliced data back together to have the full data set. We also remove schools with missing data


manhattan_adding <- slice(na.omit(cleaned_scores), 1:89)
manhattan_adding$"Income Per Capita" <- manhattan_income_per_cap
manhattan_adding$"Poverty of Borough" <- poverty_percent_manhattan
manhattan_adding$"Unemployment of Borough" <- manhattan_unemployment
#manhattan_adding

staten_adding <- slice(na.omit(cleaned_scores), 90:99)
staten_adding$"Income Per Capita" <- staten_income_per_cap
staten_adding$"Poverty of Borough" <- poverty_percent_staten
staten_adding$"Unemployment of Borough" <- staten_unemployment
#staten_adding

bronx_adding <- slice(na.omit(cleaned_scores), 100:197)
bronx_adding$"Income Per Capita" <- bronx_income_per_cap
bronx_adding$"Poverty of Borough" <- poverty_percent_bronx
bronx_adding$"Unemployment of Borough" <- bronx_unemployment
#bronx_adding

queens_adding1 <- slice(na.omit(cleaned_scores), 198:214)
queens_adding1$"Income Per Capita" <- queens_income_per_cap
queens_adding1$"Poverty of Borough" <- poverty_percent_queens
queens_adding1$"Unemployment of Borough" <- queens_unemployment
#queens_adding1

brooklyn_adding <- slice(na.omit(cleaned_scores), 215:323)
brooklyn_adding$"Income Per Capita" <- brooklyn_income_per_cap
brooklyn_adding$"Poverty of Borough" <- poverty_percent_brooklyn
brooklyn_adding$"Unemployment of Borough" <- brooklyn_unemployment
#brooklyn_adding

queens_adding2 <- slice(na.omit(cleaned_scores), 324:435)
queens_adding2$"Income Per Capita" <- queens_income_per_cap
queens_adding2$"Poverty of Borough" <- poverty_percent_queens
queens_adding2$"Unemployment of Borough" <- queens_unemployment
#queens_adding2

combined_df <- rbind(manhattan_adding, staten_adding, bronx_adding, queens_adding1, brooklyn_adding, queens_adding2)
#combined_df
```

```{r include=FALSE}

# Changing columns to be numeric and remove percent signs

percent_to_num <- function(x){
  x_replace_percent <-sub("%", "", x)
  x_as_numeric <- as.numeric(x_replace_percent)
}
combined_df[["Percent White"]] = percent_to_num(combined_df[["Percent White"]])
combined_df[["Percent Black"]] = percent_to_num(combined_df[["Percent Black"]])
combined_df[["Percent Asian"]] = percent_to_num(combined_df[["Percent Asian"]])
combined_df[["Percent Hispanic"]] = percent_to_num(combined_df[["Percent Hispanic"]])
combined_df[["Percent Tested"]] = percent_to_num(combined_df[["Percent Tested"]])
#combined_df
```

```{r include=FALSE}
math_model <- lm(combined_df$"Average Math Score" ~ combined_df$"Percent White" + combined_df$"Percent Black" + combined_df$"Percent Hispanic" + combined_df$"Percent Tested" + combined_df$"Income Per Capita" + combined_df$"Unemployment of Borough", data=combined_df)

writing_model <- lm(combined_df$"Average Writing Score" ~ combined_df$"Percent White" + combined_df$"Percent Black" + combined_df$"Percent Hispanic" + combined_df$"Percent Asian" + combined_df$"Percent Tested" + combined_df$"Income Per Capita" + combined_df$"Unemployment of Borough" + combined_df$"Poverty of Borough", data=combined_df)

reading_model <- lm(combined_df$"Average Reading Score" ~ combined_df$"Percent White" + combined_df$"Percent Black" + combined_df$"Percent Hispanic" + combined_df$"Percent Asian" + combined_df$"Percent Tested" + combined_df$"Income Per Capita" + combined_df$"Unemployment of Borough" + combined_df$"Poverty of Borough", data=combined_df)
```

```{r include=FALSE}

#These are graphs for using later

math_vs_income<-ggplot(combined_df, aes(x=combined_df$"Income Per Capita", y=combined_df$"Average Math Score"))+geom_point()+geom_smooth(method="lm")+labs(x="Income Per Capita", y="Average Math Score", title="Income Per Capita (of borough) vs. Average Math Score")

math_vs_tested <- ggplot(combined_df, aes(x=combined_df$"Percent Tested", y=combined_df$"Average Math Score"))+geom_point()+geom_smooth(method="lm")+labs(x="Percent Tested", y="Average Math Score", title="Percent Tested vs Average Math Score")

math_vs_poverty <- ggplot(combined_df, aes(x=combined_df$"Poverty of Borough", y=combined_df$"Average Math Score"))+geom_point()+geom_smooth(method="lm")+labs(x="Poverty of Borough", y="Average Math Score", title="Poverty of Borough vs. Average Math Score")

math_vs_black <- ggplot(combined_df, aes(x=combined_df$"Percent Black", y=combined_df$"Average Math Score"))+geom_point()+geom_smooth(method="lm")+labs(x="Percent Black", y="Average Math Score", title="Percent Black vs. Average Math Score")

math_vs_hispanic <- ggplot(combined_df, aes(x=combined_df$"Percent Hispanic", y=combined_df$"Average Math Score"))+geom_point()+geom_smooth(method="lm")+labs(x="Percent Hispanic", y="Average Math Score", title="Percent Hispanic vs. Average Math Score")
```
For the data I wanted to find out if there were ways to find out the factors that effect SAT scores. To do this, I found a data set of 2015 New York City SAT scores, which included information about each school, such as SAT scores for math, writing and reading, demographics of the school (percentage of kids that are white, Hispanic, black and Asian), percentage of kids tested, and a ton of location/school information that wasn't important in how it would effect these scores. I then decided to find another data set that had a bunch of New York City census results from 2015 as well. I decided to add some information that I thought might be significant in rises and falls in the scores on the SATs. Unfortunately, the census takes information into either boroughs which I felt was a little too big and census tracts that are usually up to 7,500 people and not sorted by zip code or anything else. So I had to use the borough information collected from all the census tracts in that borough. From there I found the average income of the borough using $Income_{Borough} = \frac{\Sigma Income_{Tract}*Population_{Tract}}{Population_{Borough}}$, I found the poverty percentage of the borough using $Poverty_{Borough} = \frac{\Sigma Poverty_{Tract}*Population_{Tract}}{Population_{Borough}}$ and lastly found the unemployment rate of the borough using $Unemployment_{Borough} = \frac{\Sigma Unemployment_{Tract}*Population_{Tract}}{Population_{Borough}}$. From there I combined the two data sets by splitting the SAT scores by borough, removing the rows with missing information and then adding in the information, which was the same for all schools in that borough. Then I added the rows back onto each other and had a data set with all the information I needed. The data set ended up looking like:

```{r echo=FALSE}
head(combined_df, n=10)
```

Once I was able to add in the columns from the census data (which took a lot more effort and time than I would like to admit), I could find the information which was important and significant from these variables. Of course, now that we were looking to find trends in all three of the score variables, we have three different variables to try to predict. To do this we use a linear model to find a model that represents our data the best. For the math variable, two of the variables that we have in the data set are insignificant, which are "percent Asian" and "Poverty Rate of Borough" which both did not help our model enough to use. Math scores ended up looking like:
```{r echo=FALSE}
summary(math_model)
```
The other two scores, all of the variables used were significant and the model was made better by using them, so their models looked like this for writing:
```{r echo=FALSE}
summary(writing_model)
```
and this for reading:
```{r echo=FALSE}
summary(reading_model)
```
From these three models, the important information is in the p value which the smaller the number, the more significant the variable is. This is how we removed "Poverty of Borough" and "Percent Asian" from the math data, but every other shows to be significant in all the models. The next important number from these summaries is the $R^2$ and adjusted $R^2$, which tell us how much of the variability of the estimates can be taken from the variables we used. We have an $R^2$ of 73.62% in the math model, 65.25% in the writing model and 63.32% in the reading model, which show a lot of the variability is covered by the data we used. It can't account for everything, but these are good values to have gotten. The last numbers are how these variables are related to the outcome. Finding their relationship is important and we find some interesting information from them. For all of the demographic variables, in all three of the models, we see something interesting. As all of these percentages rise, the scores on each of the SAT tests fall. We know this by seeing the negative relationships in all of the variables, which doesn't seem to make any sense for the data. But I think what this means is that diversity in schools leads to better test scores. Having a large number of any one race leads to lower test scores in general, and the more diverse a school is the better. Next, we get a positive relationship between percent tested and income per capita, which seems to make sense since the less a student has to worry about wealth and problems associated with that, the more they would be able to focus on school. Also a school encouraging a higher percentage of their kids to take the SATs will focus on the importance and help kids succeed which is shown from that relationship. Poverty of the borough has a negative relationship, so the higher rate of poverty in a borough, the lower the scores on the SATs. The last relationship is the only one that doesn't make a ton of sense, and that is the positive relationship between unemployment and test scores. Consistently, the higher the unemployment rate of a borough, the higher the test scores. This may be due to a lack of data or an inability to break up boroughs into zip codes for the census data, but it is the only one that doesn't make a ton of sense in my mind. One way we can check that the variables we measured are going to be useful is to check the confidence intervals of the variables. Essentially what we are looking for in the 3 sets of confidence intervals below is small gaps between the sets of numbers and for them to not cross from negative to positive numbers.

```{r echo=FALSE}
confint(math_model)
confint(writing_model)
confint(reading_model)
```

When looking at these intervals we got what we wanted. None of the negative relationships cross into positive numbers, and the same vice versa. We also see pretty small gaps between the numbers showing that our model will work for most sets of data.

Essentially for this data, we were able to find a lot of the variability in the SAT scores and find some interesting insights into what can help increase test scores. Obviously money and help from the school in encouraging testing will lead to higher scores as shown here:
```{r echo=FALSE}
math_vs_income
math_vs_tested
```
Sorry the income graph looks a little weird since the income variables are based off the average income of the borough so all the points are under 5 different incomes. You can still see the positive relationship, just like the graph of percent tested and math scores. Now the variables that had a negative relationship were any of the demographic percentages and poverty rate as shown here:
```{r echo=FALSE}
math_vs_poverty
math_vs_black
math_vs_hispanic
```
After looking into this subject you can see some of the variables and how they effect the SAT scores of a school. If I were to do this again in a perfect world I would have information based off zip code so we could narrow in more closely on the the different areas and find their rates of poverty and income on a smaller scale. But overall everything should be here.

(Disclaimer: if you want to see everything that was done for this project, open this in RMD and see all the steps taken to combine data frames and check information. Not all of it was important and useful, but there is a lot more there than just this write up. Thanks!)

